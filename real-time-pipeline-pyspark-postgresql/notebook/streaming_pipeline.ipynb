{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00828afb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    IntegerType, StringType,\n",
    "    TimestampType, DoubleType\n",
    ")\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aea35467",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR: /home/pazzoti/de-upskilling/real-time-pipeline-pyspark-postgresql\n",
      "DATA_DIR: /home/pazzoti/de-upskilling/real-time-pipeline-pyspark-postgresql/data/raw\n",
      "CONFIG_PATH: /home/pazzoti/de-upskilling/real-time-pipeline-pyspark-postgresql/config/postgres_connection.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BASE_DIR = \"/home/pazzoti/de-upskilling/real-time-pipeline-pyspark-postgresql\"\n",
    "\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\", \"raw\")\n",
    "CONFIG_PATH = os.path.join(BASE_DIR, \"config\", \"postgres_connection.txt\")\n",
    "\n",
    "print(\"BASE_DIR:\", BASE_DIR)\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"CONFIG_PATH:\", CONFIG_PATH)\n",
    "\n",
    "assert os.path.exists(DATA_DIR), \" data/raw folder not found\"\n",
    "assert os.path.exists(CONFIG_PATH), \" postgres_connection.txt not found\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e6b70ba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DB Config Loaded:\n",
      "localhost 5432 ecommerce_db postgres\n"
     ]
    }
   ],
   "source": [
    "config = {}\n",
    "\n",
    "with open(CONFIG_PATH) as f:\n",
    "    for line in f:\n",
    "        if \"=\" in line:\n",
    "            k, v = line.strip().split(\"=\")\n",
    "            config[k] = v\n",
    "\n",
    "host = config[\"host\"]\n",
    "port = config[\"port\"]\n",
    "database = config[\"database\"]\n",
    "user = config[\"user\"]\n",
    "password = config[\"password\"]\n",
    "\n",
    "print(\"âœ… DB Config Loaded:\")\n",
    "print(host, port, database, user)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "874f4a89",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"action\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0dcbed9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/01/28 18:25:00 WARN Utils: Your hostname, Patricks-bot, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "26/01/28 18:25:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/pazzoti/de-upskilling/real-time-pipeline-pyspark-postgresql/venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/pazzoti/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/pazzoti/.ivy2.5.2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ebf5bb9a-e37c-430c-867d-20220cb4c293;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.7.3 in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      ":: resolution report :: resolve 152ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\torg.checkerframework#checker-qual;3.42.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ebf5bb9a-e37c-430c-867d-20220cb4c293\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/6ms)\n",
      "26/01/28 18:25:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Spark started\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RealTimeEcommercePipeline\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "print(\"âœ… Spark started\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ac5b0eb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_stream = spark.readStream \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(schema) \\\n",
    "    .csv(DATA_DIR)\n",
    "\n",
    "df_stream.printSchema()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e17dd3ad-ab1e-412f-9c97-20cedc6b9519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… JDBC WRITE TEST SUCCESS\n"
     ]
    }
   ],
   "source": [
    "test_df = spark.createDataFrame(\n",
    "    [(1, 101, \"test\", datetime.now(), 9.99, \"debug\")],\n",
    "    [\"user_id\", \"product_id\", \"action\", \"timestamp\", \"price\", \"category\"]\n",
    ")\n",
    "\n",
    "jdbc_url = f\"jdbc:postgresql://{host}:{port}/{database}\"\n",
    "\n",
    "test_df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"user_events\") \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"âœ… JDBC WRITE TEST SUCCESS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e2ad6e2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def write_to_postgres(batch_df, batch_id):\n",
    "    print(f\"ðŸš€ Writing batch {batch_id} to PostgreSQL\")\n",
    "\n",
    "    batch_df.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"dbtable\", \"user_events\") \\\n",
    "        .option(\"user\", user) \\\n",
    "        .option(\"password\", password) \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "213a9e8a-bdf4-4c86-b38d-c932240c573c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Streaming started\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_DIR = os.path.join(BASE_DIR, \"checkpoints\")\n",
    "\n",
    "query = df_stream.writeStream \\\n",
    "    .foreachBatch(write_to_postgres) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_DIR) \\\n",
    "    .start()\n",
    "\n",
    "print(\"âœ… Streaming started\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f5693b5-ff6a-4c34-bf50-a0db60206abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Initializing sources',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.status\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
